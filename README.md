# Fairness in Recidivism Risk Assessments

This project considers the use of recidivism risk assessments derived from machine learning algorithms and their fairness. In particular, we study the data collected by ProPublica to examine the COMPAS algorithm and apply doubly-robust counterfactual methods developed by [Coston et al.](https://arxiv.org/abs/1909.00066) to understand the fairness of recidivism risk assessments. 

# Background

Risk assessment instruments are increasingly being used to help guide and improve decision making in the criminal justice system. These instruments build risk scores, often via machine learning, for the likelihood of some adverse outcome occuring given available data on an individual. For example, risk assessment instruments have been used to predict recidivism among people accused of a crime, which then is used to inform bail and parole decisions. The overarching goal in this scenario is to predict whether someone accused of a crime, if released, will commit another crime within a fixed period of time. These predictions are then used by decision-makers (e.g. judges) to make interventions to mitigate the risk of recidivism. 

There are a number of issues that arise from the use of these risk assessment instruments. The United States already has a mass-incarceration crisis, but to make matters worse, minority populations are the ones disproportionately affected by it. It is possible that risk assessment instruments only exacerbate the disparities that already exist in the criminal justice system. In addition, we do not have data on who commits crimes, we only have data on who is accused of a crime and arrested. There is evidence to suggest that arrest data is skewed towards minority populations by being policed at a higher rate, meaning any machine learning algorithms trained on such data will replicate any such bias present in the data. 

To address these issues, we apply the methods developed by [Coston et al.](https://arxiv.org/abs/1909.00066) to our setting of evaluating recidivism risk assessments and understanding the probabilities of recidivism under proposed decisions. As the authors state, these issues suggest a counterfactual approach as opposed to one of pure prediction. Risk assessment instruments trained to predict recidivism using the historical arrest data answer the question: What is the likelihood of recidivism under the observed historical decisions? Yet, the question of relevance for decision-makers is: What is the likelihood of recidivism under the proposed decision? When the decisions do not impact outcomes, these two questions are equivalent. However, the decision to release persons accused of a crime pretrial clearly affects the likelihood of recidivism, meaning the two will likely differ. Risk assessment instruments must take into account the effects of the interventions as they might appear to perform well according to standard prediction metrics but perform poorly for cases such as those that have historically been receptive to the intervention. 

# Data

We utilize the [data](https://github.com/propublica/compas-analysis) ProPublica obtained from the Broward County Sheriff's Office in Florida to examine the COMPAS (Correctional Offender Management Profiling For Alternative Sanctions) recidivism tool. It includes every defendant that received a COMPAS scores in Broward County from 2013 to 2014. The dataset included variables such as severity of charge, number of priors, demographics, age, sex, the COMPAS scores, and whether each person was accused of a crime within two years. The COMPAS scores for each person range from 1 to 10, with ten being the highest risk. Scores 1 to 4 were considered low risk, 5 to 7 were medium risk, and 8 to 10 were high risk. 

More detail on the data is available on ProPublica's [post](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm).

# Methods

## Counterfactual definitions

Let <img src="/tex/42097967b35a887677417731d387566b.svg?invert_in_darkmode&sanitize=true" align=middle width=73.47023804999998pt height=24.65753399999998pt/> denote the observed outcome, where <img src="/tex/a9ac97051ce94e9fdf240c6df4dc795b.svg?invert_in_darkmode&sanitize=true" align=middle width=43.33321904999999pt height=22.465723500000017pt/> indicates a new arrest within two years and <img src="/tex/1259eaa4c1468d01a33acf332c3a49d2.svg?invert_in_darkmode&sanitize=true" align=middle width=43.33321904999999pt height=22.465723500000017pt/> indicates no new arrest. Let <img src="/tex/a92c57066b77406c864f49464637b892.svg?invert_in_darkmode&sanitize=true" align=middle width=72.60265649999998pt height=24.65753399999998pt/> denote the binary decision (or treatment/intervention in a classic causal inference sense) made based on COMPAS risk assessment algorithm. Note that in practice, the decision may not be purely binary (different levels of parole/bail decisions). In addition, we make the simplifying assumption that all 1 to 5 COMPAS scores were assigned <img src="/tex/3efd8f56b9a4de7cd1da2f06d49c6a54.svg?invert_in_darkmode&sanitize=true" align=middle width=42.46563914999999pt height=22.465723500000017pt/> and all 6 to 10 COMPAS scores were assigned <img src="/tex/7ae5dc3f691fecdab2448dbbbae6186a.svg?invert_in_darkmode&sanitize=true" align=middle width=42.46563914999999pt height=22.465723500000017pt/>. Thus, we then have that <img src="/tex/6b08ac66740aa592612db9ccd3ee9cf4.svg?invert_in_darkmode&sanitize=true" align=middle width=23.08229054999999pt height=27.6567522pt/> denote the counterfactual outcomes, or the outcomes under decision <img src="/tex/a92c57066b77406c864f49464637b892.svg?invert_in_darkmode&sanitize=true" align=middle width=72.60265649999998pt height=24.65753399999998pt/>, of which we only observe one. We consider <img src="/tex/3efd8f56b9a4de7cd1da2f06d49c6a54.svg?invert_in_darkmode&sanitize=true" align=middle width=42.46563914999999pt height=22.465723500000017pt/> to be the baseline decision of no intervention, the one under which it is relevant to assess the risk of the outcome. It is of interest to assess the risk of recidivism if subjects are released pretrial, for instance. Finally, let <img src="/tex/d05b996d2c08252f77613c25205a0f04.svg?invert_in_darkmode&sanitize=true" align=middle width=14.29216634999999pt height=22.55708729999998pt/> denote the matrix of features used to calculate recidivism risk. 

## Learning models

The observational learning model produces recidivism risk assessments under the observed historical decisions. Statistically, this is equivalent to estimating <img src="/tex/d1fedded0f2a40897b2f7ba119320172.svg?invert_in_darkmode&sanitize=true" align=middle width=56.64394724999998pt height=24.65753399999998pt/> and can be done by training any appropriate classifier to the observed population. 

The counterfactual learning model produces recidivism risk assessments under the baseline decision. Statistically, this is equivalent to estimating <img src="/tex/e515709a937edf8f448359b4a1aace6b.svg?invert_in_darkmode&sanitize=true" align=middle width=64.01840609999998pt height=26.76175259999998pt/>. Although we do not observe <img src="/tex/84d282b56c7d0e96ad3f0a58fafb8626.svg?invert_in_darkmode&sanitize=true" align=middle width=19.748925899999993pt height=26.76175259999998pt/> for all subjects, this model can still be used to obtain risk estimates for all subjects under the following identification assumptions: Consistency, no unmeasured confounding, and weak positivity. Under these assumptions, our estimand can be written as <img src="/tex/01846363809d0078965c98ef17a899d2.svg?invert_in_darkmode&sanitize=true" align=middle width=191.43826019999997pt height=26.76175259999998pt/>, meaning we can train any appropriate classifier to the population that received <img src="/tex/3efd8f56b9a4de7cd1da2f06d49c6a54.svg?invert_in_darkmode&sanitize=true" align=middle width=42.46563914999999pt height=22.465723500000017pt/>. 

## Evaluation

Ultimately, we want the recidivism risk assessment models to be most informative towards decision-making where it makes sense to target interventions for those most likely to commit another crime. To evaluate whether the ability of the observational and counterfactual learning models to do, we calculate Precision-Recall (PR) and Receiver Operative Characteristic (ROC) curves. The PR curve plots precision against recall and the ROC curve plots recall against the false positive rate (FPR). 

Let <img src="/tex/a68518ef5c8caf2b2ff4ff10e00fa9ea.svg?invert_in_darkmode&sanitize=true" align=middle width=73.47021989999999pt height=31.141535699999984pt/> be the predicted recidivism status from either the observational or counterfactual learning model. Then the observational PR curve plots observational precision, <img src="/tex/23d899f66358e9360b17d79ef1711f2f.svg?invert_in_darkmode&sanitize=true" align=middle width=85.06848074999998pt height=31.141535699999984pt/>, against observational recall, <img src="/tex/4e27f350be3148519215debb66d75a9b.svg?invert_in_darkmode&sanitize=true" align=middle width=85.06848074999998pt height=31.141535699999984pt/>. The observational ROC curve plots observational recall against observational FPR, <img src="/tex/087de6e31e6e6506c283fd8bad9ddaad.svg?invert_in_darkmode&sanitize=true" align=middle width=85.06848074999998pt height=31.141535699999984pt/>.

The counterfactual PR curve plots counterfactual precision, <img src="/tex/edb8769dfb68f1d8c803b297d19d933d.svg?invert_in_darkmode&sanitize=true" align=middle width=92.44294124999999pt height=31.141535699999984pt/>, against observational recall, <img src="/tex/edc4d03083efb582ea2e0b6243ef10e3.svg?invert_in_darkmode&sanitize=true" align=middle width=92.44294124999999pt height=31.141535699999984pt/>. The counterfactual ROC curve plots counterfactual recall against counterfactual FPR, <img src="/tex/7ad43f013e050278acd239ba043ef877.svg?invert_in_darkmode&sanitize=true" align=middle width=92.44294124999999pt height=31.141535699999984pt/>. The counterfactual metrics can all be identified by the observed data under the given causal assumptions described above. Estimation of the counterfactual metrics is done via doubly-robust estimation, which have a number of advantages over outcome regression or inverse probability weighting estimates. In particular, the counterfactual metrics require estimating the regression models <img src="/tex/e515709a937edf8f448359b4a1aace6b.svg?invert_in_darkmode&sanitize=true" align=middle width=64.01840609999998pt height=26.76175259999998pt/> and <img src="/tex/a15221037ec6ba45c6573f27f475eda7.svg?invert_in_darkmode&sanitize=true" align=middle width=85.91320484999999pt height=24.65753399999998pt/>, the propensity score. In practice, it is likely that the true functional forms of these models in unknown, so we would therefore like to model them as flexibly as possibly using non-parametric machine learning algorithms. Doubly-robust estimators have faster rates of convergence when using non-parametric methods and are robust to misspecification of one of the regression models. 

## Prediction models

We utilized gradient boosted trees to fit the regression models <img src="/tex/e515709a937edf8f448359b4a1aace6b.svg?invert_in_darkmode&sanitize=true" align=middle width=64.01840609999998pt height=26.76175259999998pt/> and	<img src="/tex/a15221037ec6ba45c6573f27f475eda7.svg?invert_in_darkmode&sanitize=true" align=middle width=85.91320484999999pt height=24.65753399999998pt/>.

# Feature engineering

We build a SQL-based feature engineering step to extract features for the prediction algorithms. Performing feature engineering on top of existing SQL databases allows for a scalable and easily maintained machine learning pipeline. In addition, there are speed gains over loading all the data into pandas. Lastly, although the ProPublica data is not very large, being SQL based means a similar feature engineering pipeline can be done on clusters for big data.  

# Results